{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_1 = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "document_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Jamal', \"Khashoggi's\", 'Saudi', 'Saudis', 'The']...)\n"
     ]
    }
   ],
   "source": [
    "list_of_word = list()\n",
    "for scentence in document_1:\n",
    "    list_of_word.extend(scentence.split(' '))\n",
    "\n",
    "list_of_words  = [list_of_word]\n",
    "dictionary = corpora.Dictionary(list_of_words)\n",
    "print (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(60 unique tokens: ['Jamal', \"Khashoggi's\", 'Saudi', 'Saudis', 'The']...)\n"
     ]
    }
   ],
   "source": [
    "list_of_word_2 = list()\n",
    "for scentence in document_2:\n",
    "    list_of_word_2.extend(scentence.split(' '))\n",
    "\n",
    "list_of_words_2 = [list_of_word_2]\n",
    "dictionary.add_documents(list_of_words_2)\n",
    "print (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jamal': 0,\n",
       " \"Khashoggi's\": 1,\n",
       " 'Saudi': 2,\n",
       " 'Saudis': 3,\n",
       " 'The': 4,\n",
       " 'Turkey,': 5,\n",
       " 'a': 6,\n",
       " 'abduction': 7,\n",
       " 'according': 8,\n",
       " 'acknowledge': 9,\n",
       " 'an': 10,\n",
       " 'are': 11,\n",
       " 'death': 12,\n",
       " 'from': 13,\n",
       " 'his': 14,\n",
       " 'intended': 15,\n",
       " 'interrogation': 16,\n",
       " 'journalist': 17,\n",
       " 'lead': 18,\n",
       " 'of': 19,\n",
       " 'one': 20,\n",
       " 'preparing': 21,\n",
       " 'report': 22,\n",
       " 'result': 23,\n",
       " 'sources.': 24,\n",
       " 'that': 25,\n",
       " 'the': 26,\n",
       " 'to': 27,\n",
       " 'two': 28,\n",
       " 'was': 29,\n",
       " 'went': 30,\n",
       " 'will': 31,\n",
       " 'wrong,': 32,\n",
       " 'One': 33,\n",
       " 'acknowledged': 34,\n",
       " 'and': 35,\n",
       " 'be': 36,\n",
       " 'being': 37,\n",
       " 'carried': 38,\n",
       " 'cautioned': 39,\n",
       " 'change.': 40,\n",
       " 'clearance': 41,\n",
       " 'conclude': 42,\n",
       " 'could': 43,\n",
       " 'held': 44,\n",
       " 'involved': 45,\n",
       " 'is': 46,\n",
       " 'likely': 47,\n",
       " 'operation': 48,\n",
       " 'out': 49,\n",
       " 'prepared': 50,\n",
       " 'responsible.': 51,\n",
       " 'says': 52,\n",
       " 'source': 53,\n",
       " 'sources': 54,\n",
       " 'still': 55,\n",
       " 'things': 56,\n",
       " 'those': 57,\n",
       " 'transparency': 58,\n",
       " 'without': 59}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess \n",
    "from smart_open import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary_file = corpora.Dictionary(simple_preprocess(scentence, deacc=True) for scentence in smart_open(\"./NLP/Testing_smart_open-1.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 0,\n",
       " 'and': 1,\n",
       " 'articles': 2,\n",
       " 'as': 3,\n",
       " 'attracting': 4,\n",
       " 'billion': 5,\n",
       " 'by': 6,\n",
       " 'coined': 7,\n",
       " 'comprises': 8,\n",
       " 'created': 9,\n",
       " 'developed': 10,\n",
       " 'encyclopedia': 11,\n",
       " 'encyclopedias': 12,\n",
       " 'english': 13,\n",
       " 'for': 14,\n",
       " 'hawaiian': 15,\n",
       " 'in': 16,\n",
       " 'initially': 17,\n",
       " 'is': 18,\n",
       " 'its': 19,\n",
       " 'january': 20,\n",
       " 'jimmy': 21,\n",
       " 'language': 22,\n",
       " 'languages': 23,\n",
       " 'largest': 24,\n",
       " 'larry': 25,\n",
       " 'launched': 26,\n",
       " 'million': 27,\n",
       " 'month': 28,\n",
       " 'more': 29,\n",
       " 'name': 30,\n",
       " 'of': 31,\n",
       " 'on': 32,\n",
       " 'other': 33,\n",
       " 'overall': 34,\n",
       " 'per': 35,\n",
       " 'portmanteau': 36,\n",
       " 'quick': 37,\n",
       " 'quickly': 38,\n",
       " 'sanger': 39,\n",
       " 'than': 40,\n",
       " 'the': 41,\n",
       " 'unique': 42,\n",
       " 'versions': 43,\n",
       " 'visitors': 44,\n",
       " 'wales': 45,\n",
       " 'was': 46,\n",
       " 'were': 47,\n",
       " 'wiki': 48,\n",
       " 'wikipedia': 49,\n",
       " 'with': 50,\n",
       " 'words': 51,\n",
       " 'accuracy': 52,\n",
       " 'all': 53,\n",
       " 'allowing': 54,\n",
       " 'although': 55,\n",
       " 'anyone': 56,\n",
       " 'approached': 57,\n",
       " 'best': 58,\n",
       " 'biggest': 59,\n",
       " 'britannica': 60,\n",
       " 'comparing': 61,\n",
       " 'contentious': 62,\n",
       " 'critics': 63,\n",
       " 'door': 64,\n",
       " 'edit': 65,\n",
       " 'encyclop√¶dia': 66,\n",
       " 'fared': 67,\n",
       " 'focused': 68,\n",
       " 'following': 69,\n",
       " 'found': 70,\n",
       " 'from': 71,\n",
       " 'had': 72,\n",
       " 'hard': 73,\n",
       " 'have': 74,\n",
       " 'issues': 75,\n",
       " 'it': 76,\n",
       " 'level': 77,\n",
       " 'made': 78,\n",
       " 'might': 79,\n",
       " 'nature': 80,\n",
       " 'not': 81,\n",
       " 'one': 82,\n",
       " 'open': 83,\n",
       " 'or': 84,\n",
       " 'peer': 85,\n",
       " 'policy': 86,\n",
       " 'possibly': 87,\n",
       " 'published': 88,\n",
       " 'random': 89,\n",
       " 'review': 90,\n",
       " 'sampling': 91,\n",
       " 'science': 92,\n",
       " 'similar': 93,\n",
       " 'so': 94,\n",
       " 'social': 95,\n",
       " 'stated': 96,\n",
       " 'study': 97,\n",
       " 'suggested': 98,\n",
       " 'testament': 99,\n",
       " 'that': 100,\n",
       " 'time': 101,\n",
       " 'to': 102,\n",
       " 'vision': 103,\n",
       " 'well': 104,\n",
       " 'world': 105,\n",
       " 'year': 106,\n",
       " 'also': 107,\n",
       " 'announced': 108,\n",
       " 'are': 109,\n",
       " 'been': 110,\n",
       " 'being': 111,\n",
       " 'bias': 112,\n",
       " 'black': 113,\n",
       " 'controversial': 114,\n",
       " 'coverage': 115,\n",
       " 'criticized': 116,\n",
       " 'detect': 117,\n",
       " 'dominant': 118,\n",
       " 'editors': 119,\n",
       " 'edwin': 120,\n",
       " 'encourage': 121,\n",
       " 'exhibiting': 122,\n",
       " 'facebook': 123,\n",
       " 'fake': 124,\n",
       " 'falsehoods': 125,\n",
       " 'female': 126,\n",
       " 'gender': 127,\n",
       " 'half': 128,\n",
       " 'has': 129,\n",
       " 'held': 130,\n",
       " 'help': 131,\n",
       " 'however': 132,\n",
       " 'increase': 133,\n",
       " 'links': 134,\n",
       " 'majority': 135,\n",
       " 'male': 136,\n",
       " 'manipulation': 137,\n",
       " 'mixture': 138,\n",
       " 'news': 139,\n",
       " 'particularly': 140,\n",
       " 'plan': 141,\n",
       " 'presenting': 142,\n",
       " 'readers': 143,\n",
       " 'related': 144,\n",
       " 'some': 145,\n",
       " 'spin': 146,\n",
       " 'subject': 147,\n",
       " 'suggesting': 148,\n",
       " 'systemic': 149,\n",
       " 'thons': 150,\n",
       " 'topics': 151,\n",
       " 'truth': 152,\n",
       " 'version': 153,\n",
       " 'where': 154,\n",
       " 'women': 155,\n",
       " 'would': 156,\n",
       " 'youtube': 157}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_file.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create class to read all Files into a directory and yield all list of words using iter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(231 unique tokens: ['an', 'and', 'articles', 'as', 'attracting']...)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class ReadDirectory(object):\n",
    "    def __init__(self,DirPath):\n",
    "        self.DirPath = DirPath\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for file_name in os.listdir(self.DirPath):\n",
    "            for lines in smart_open(os.path.join(self.DirPath,file_name)):\n",
    "                yield simple_preprocess(lines)\n",
    "\n",
    "master_dict = corpora.Dictionary(ReadDirectory('NLP'))\n",
    "print(master_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag Of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = [master_dict.doc2bow(doc) for doc in ReadDirectory('NLP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 3), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 1), (29, 2), (30, 1), (31, 3), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (41, 4), (42, 1), (43, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 5), (50, 1), (51, 1)], [], [(1, 4), (2, 2), (11, 1), (16, 3), (21, 1), (31, 6), (32, 1), (41, 6), (45, 1), (46, 1), (49, 3), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 2), (93, 1), (94, 1), (95, 2), (96, 1), (97, 1), (98, 1), (99, 1), (100, 4), (101, 1), (102, 2), (103, 1), (104, 1), (105, 1), (106, 1)], [], [(1, 4), (2, 1), (6, 2), (13, 1), (14, 4), (16, 2), (19, 1), (22, 1), (31, 3), (32, 1), (41, 2), (49, 4), (65, 1), (74, 1), (76, 1), (93, 1), (100, 1), (102, 3), (107, 1), (108, 2), (109, 1), (110, 3), (111, 1), (112, 2), (113, 1), (114, 1), (115, 1), (116, 3), (117, 1), (118, 1), (119, 2), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 3), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 2), (152, 2), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1)], [(18, 1), (41, 2), (102, 3), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1)], [], [(1, 1), (16, 1), (18, 1), (41, 5), (82, 1), (100, 1), (101, 1), (102, 1), (139, 1), (161, 1), (162, 1), (163, 1), (171, 2), (173, 1), (174, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1)], [], [(16, 1), (145, 1), (182, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1)], [], [(1, 2), (3, 2), (14, 1), (16, 1), (18, 3), (19, 1), (31, 4), (41, 1), (51, 2), (84, 1), (100, 2), (102, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 2), (205, 1), (206, 3), (207, 1), (208, 2), (209, 1), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 3), (221, 2)], [], [(1, 1), (18, 1), (31, 1), (41, 1), (71, 1), (94, 1), (102, 1), (160, 1), (161, 1), (180, 1), (182, 1), (190, 1), (195, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrive words back from bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('an', 1), ('and', 3), ('articles', 2), ('as', 1), ('attracting', 1), ('billion', 1), ('by', 1), ('coined', 1), ('comprises', 1), ('created', 1), ('developed', 1), ('encyclopedia', 2), ('encyclopedias', 1), ('english', 2), ('for', 1), ('hawaiian', 1), ('in', 1), ('initially', 1), ('is', 1), ('its', 1), ('january', 1), ('jimmy', 1), ('language', 1), ('languages', 1), ('largest', 1), ('larry', 1), ('launched', 1), ('million', 2), ('month', 1), ('more', 2), ('name', 1), ('of', 3), ('on', 1), ('other', 1), ('overall', 1), ('per', 1), ('portmanteau', 1), ('quick', 1), ('quickly', 1), ('sanger', 2), ('than', 2), ('the', 4), ('unique', 1), ('versions', 1), ('visitors', 1), ('wales', 1), ('was', 2), ('were', 1), ('wiki', 1), ('wikipedia', 5), ('with', 1), ('words', 1)], [], [('and', 4), ('articles', 2), ('encyclopedia', 1), ('in', 3), ('jimmy', 1), ('of', 6), ('on', 1), ('the', 6), ('wales', 1), ('was', 1), ('wikipedia', 3), ('accuracy', 1), ('all', 1), ('allowing', 1), ('although', 1), ('anyone', 1), ('approached', 1), ('best', 1), ('biggest', 1), ('britannica', 2), ('comparing', 1), ('contentious', 1), ('critics', 1), ('door', 1), ('edit', 1), ('encyclop√¶dia', 1), ('fared', 1), ('focused', 1), ('following', 1), ('found', 1), ('from', 1), ('had', 1), ('hard', 1), ('have', 1), ('issues', 1), ('it', 1), ('level', 1), ('made', 1), ('might', 1), ('nature', 1), ('not', 1), ('one', 1), ('open', 1), ('or', 2), ('peer', 1), ('policy', 1), ('possibly', 1), ('published', 1), ('random', 1), ('review', 1), ('sampling', 1), ('science', 2), ('similar', 1), ('so', 1), ('social', 2), ('stated', 1), ('study', 1), ('suggested', 1), ('testament', 1), ('that', 4), ('time', 1), ('to', 2), ('vision', 1), ('well', 1), ('world', 1), ('year', 1)], [], [('and', 4), ('articles', 1), ('by', 2), ('english', 1), ('for', 4), ('in', 2), ('its', 1), ('language', 1), ('of', 3), ('on', 1), ('the', 2), ('wikipedia', 4), ('edit', 1), ('have', 1), ('it', 1), ('similar', 1), ('that', 1), ('to', 3), ('also', 1), ('announced', 2), ('are', 1), ('been', 3), ('being', 1), ('bias', 2), ('black', 1), ('controversial', 1), ('coverage', 1), ('criticized', 3), ('detect', 1), ('dominant', 1), ('editors', 2), ('edwin', 1), ('encourage', 1), ('exhibiting', 1), ('facebook', 1), ('fake', 1), ('falsehoods', 1), ('female', 1), ('gender', 1), ('half', 1), ('has', 3), ('held', 1), ('help', 1), ('however', 1), ('increase', 1), ('links', 1), ('majority', 1), ('male', 1), ('manipulation', 1), ('mixture', 1), ('news', 1), ('particularly', 1), ('plan', 1), ('presenting', 1), ('readers', 1), ('related', 1), ('some', 1), ('spin', 1), ('subject', 1), ('suggesting', 1), ('systemic', 1), ('thons', 1), ('topics', 2), ('truth', 2), ('version', 1), ('where', 1), ('women', 1), ('would', 1), ('youtube', 1)], [('is', 1), ('the', 2), ('to', 3), ('able', 1), ('be', 1), ('create', 1), ('dictionary', 1), ('entire', 1), ('file', 1), ('having', 1), ('input', 1), ('large', 1), ('load', 1), ('need', 1), ('now', 1), ('object', 1), ('text', 2), ('when', 1), ('without', 1), ('you', 1), ('your', 1)], [], [('and', 1), ('in', 1), ('is', 1), ('the', 5), ('one', 1), ('that', 1), ('time', 1), ('to', 1), ('news', 1), ('dictionary', 1), ('entire', 1), ('file', 1), ('text', 2), ('without', 1), ('you', 1), ('at', 1), ('do', 1), ('gensim', 1), ('good', 1), ('how', 1), ('into', 1), ('let', 1), ('lets', 1), ('line', 1), ('loading', 1), ('memory', 1), ('next', 1), ('read', 1), ('sections', 1), ('see', 1), ('system', 1), ('update', 1)], [], [('in', 1), ('some', 1), ('let', 1), ('before', 1), ('but', 1), ('get', 1), ('jargon', 1), ('nlp', 1), ('understand', 1), ('we', 1)], [], [('and', 2), ('as', 2), ('for', 1), ('in', 1), ('is', 3), ('its', 1), ('of', 4), ('the', 1), ('words', 2), ('or', 1), ('that', 2), ('to', 1), ('bag', 1), ('can', 1), ('collection', 1), ('contains', 1), ('corpus', 2), ('count', 1), ('document', 3), ('documents', 1), ('each', 2), ('frequency', 1), ('id', 1), ('information', 1), ('lost', 1), ('means', 1), ('order', 1), ('paragraph', 1), ('refer', 1), ('result', 1), ('sentence', 1), ('token', 1), ('typically', 3), ('word', 2)], [], [('and', 1), ('is', 1), ('of', 1), ('the', 1), ('from', 1), ('so', 1), ('to', 1), ('create', 1), ('dictionary', 1), ('how', 1), ('let', 1), ('see', 1), ('get', 1), ('clear', 1), ('everything', 1), ('far', 1), ('hands', 1), ('if', 1), ('list', 1), ('our', 1), ('sentences', 1), ('wet', 1)]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = [[(master_dict[id], count) for id, count in line] for line in Corpus]\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dictionary & corpus ( bag of Words ) to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dict.save('master_dict.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize('Corpus.mm', Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dict & corpus from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(231 unique tokens: ['an', 'and', 'articles', 'as', 'attracting']...)\n"
     ]
    }
   ],
   "source": [
    "dict_copy = corpora.Dictionary.load('master_dict.dict')\n",
    "print(dict_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 3.0), (2, 2.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 2.0), (12, 1.0), (13, 2.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0), (27, 2.0), (28, 1.0), (29, 2.0), (30, 1.0), (31, 3.0), (32, 1.0), (33, 1.0), (34, 1.0), (35, 1.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 2.0), (40, 2.0), (41, 4.0), (42, 1.0), (43, 1.0), (44, 1.0), (45, 1.0), (46, 2.0), (47, 1.0), (48, 1.0), (49, 5.0), (50, 1.0), (51, 1.0)]\n",
      "[]\n",
      "[(1, 4.0), (2, 2.0), (11, 1.0), (16, 3.0), (21, 1.0), (31, 6.0), (32, 1.0), (41, 6.0), (45, 1.0), (46, 1.0), (49, 3.0), (52, 1.0), (53, 1.0), (54, 1.0), (55, 1.0), (56, 1.0), (57, 1.0), (58, 1.0), (59, 1.0), (60, 2.0), (61, 1.0), (62, 1.0), (63, 1.0), (64, 1.0), (65, 1.0), (66, 1.0), (67, 1.0), (68, 1.0), (69, 1.0), (70, 1.0), (71, 1.0), (72, 1.0), (73, 1.0), (74, 1.0), (75, 1.0), (76, 1.0), (77, 1.0), (78, 1.0), (79, 1.0), (80, 1.0), (81, 1.0), (82, 1.0), (83, 1.0), (84, 2.0), (85, 1.0), (86, 1.0), (87, 1.0), (88, 1.0), (89, 1.0), (90, 1.0), (91, 1.0), (92, 2.0), (93, 1.0), (94, 1.0), (95, 2.0), (96, 1.0), (97, 1.0), (98, 1.0), (99, 1.0), (100, 4.0), (101, 1.0), (102, 2.0), (103, 1.0), (104, 1.0), (105, 1.0), (106, 1.0)]\n",
      "[]\n",
      "[(1, 4.0), (2, 1.0), (6, 2.0), (13, 1.0), (14, 4.0), (16, 2.0), (19, 1.0), (22, 1.0), (31, 3.0), (32, 1.0), (41, 2.0), (49, 4.0), (65, 1.0), (74, 1.0), (76, 1.0), (93, 1.0), (100, 1.0), (102, 3.0), (107, 1.0), (108, 2.0), (109, 1.0), (110, 3.0), (111, 1.0), (112, 2.0), (113, 1.0), (114, 1.0), (115, 1.0), (116, 3.0), (117, 1.0), (118, 1.0), (119, 2.0), (120, 1.0), (121, 1.0), (122, 1.0), (123, 1.0), (124, 1.0), (125, 1.0), (126, 1.0), (127, 1.0), (128, 1.0), (129, 3.0), (130, 1.0), (131, 1.0), (132, 1.0), (133, 1.0), (134, 1.0), (135, 1.0), (136, 1.0), (137, 1.0), (138, 1.0), (139, 1.0), (140, 1.0), (141, 1.0), (142, 1.0), (143, 1.0), (144, 1.0), (145, 1.0), (146, 1.0), (147, 1.0), (148, 1.0), (149, 1.0), (150, 1.0), (151, 2.0), (152, 2.0), (153, 1.0), (154, 1.0), (155, 1.0), (156, 1.0), (157, 1.0)]\n",
      "[(18, 1.0), (41, 2.0), (102, 3.0), (158, 1.0), (159, 1.0), (160, 1.0), (161, 1.0), (162, 1.0), (163, 1.0), (164, 1.0), (165, 1.0), (166, 1.0), (167, 1.0), (168, 1.0), (169, 1.0), (170, 1.0), (171, 2.0), (172, 1.0), (173, 1.0), (174, 1.0), (175, 1.0)]\n",
      "[]\n",
      "[(1, 1.0), (16, 1.0), (18, 1.0), (41, 5.0), (82, 1.0), (100, 1.0), (101, 1.0), (102, 1.0), (139, 1.0), (161, 1.0), (162, 1.0), (163, 1.0), (171, 2.0), (173, 1.0), (174, 1.0), (176, 1.0), (177, 1.0), (178, 1.0), (179, 1.0), (180, 1.0), (181, 1.0), (182, 1.0), (183, 1.0), (184, 1.0), (185, 1.0), (186, 1.0), (187, 1.0), (188, 1.0), (189, 1.0), (190, 1.0), (191, 1.0), (192, 1.0)]\n",
      "[]\n",
      "[(16, 1.0), (145, 1.0), (182, 1.0), (193, 1.0), (194, 1.0), (195, 1.0), (196, 1.0), (197, 1.0), (198, 1.0), (199, 1.0)]\n",
      "[]\n",
      "[(1, 2.0), (3, 2.0), (14, 1.0), (16, 1.0), (18, 3.0), (19, 1.0), (31, 4.0), (41, 1.0), (51, 2.0), (84, 1.0), (100, 2.0), (102, 1.0), (200, 1.0), (201, 1.0), (202, 1.0), (203, 1.0), (204, 2.0), (205, 1.0), (206, 3.0), (207, 1.0), (208, 2.0), (209, 1.0), (210, 1.0), (211, 1.0), (212, 1.0), (213, 1.0), (214, 1.0), (215, 1.0), (216, 1.0), (217, 1.0), (218, 1.0), (219, 1.0), (220, 3.0), (221, 2.0)]\n",
      "[]\n",
      "[(1, 1.0), (18, 1.0), (31, 1.0), (41, 1.0), (71, 1.0), (94, 1.0), (102, 1.0), (160, 1.0), (161, 1.0), (180, 1.0), (182, 1.0), (190, 1.0), (195, 1.0), (222, 1.0), (223, 1.0), (224, 1.0), (225, 1.0), (226, 1.0), (227, 1.0), (228, 1.0), (229, 1.0), (230, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "corpus_copy = corpora.MmCorpus('Corpus.mm')\n",
    "for line in corpus_copy:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "tfidf = models.TfidfModel(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('an', 0.12), ('and', 0.12), ('articles', 0.14), ('as', 0.09), ('attracting', 0.12), ('billion', 0.12), ('by', 0.09), ('coined', 0.12), ('comprises', 0.12), ('created', 0.12), ('developed', 0.12), ('encyclopedia', 0.18), ('encyclopedias', 0.12), ('english', 0.18), ('for', 0.07), ('hawaiian', 0.12), ('in', 0.04), ('initially', 0.12), ('is', 0.05), ('its', 0.07), ('january', 0.12), ('jimmy', 0.09), ('language', 0.09), ('languages', 0.12), ('largest', 0.12), ('larry', 0.12), ('launched', 0.12), ('million', 0.24), ('month', 0.12), ('more', 0.24), ('name', 0.12), ('of', 0.14), ('on', 0.07), ('other', 0.12), ('overall', 0.12), ('per', 0.12), ('portmanteau', 0.12), ('quick', 0.12), ('quickly', 0.12), ('sanger', 0.24), ('than', 0.24), ('the', 0.13), ('unique', 0.12), ('versions', 0.12), ('visitors', 0.12), ('wales', 0.09), ('was', 0.18), ('were', 0.12), ('wiki', 0.12), ('wikipedia', 0.35), ('with', 0.12), ('words', 0.09)], [], [('and', 0.14), ('articles', 0.13), ('encyclopedia', 0.08), ('in', 0.11), ('jimmy', 0.08), ('of', 0.26), ('on', 0.06), ('the', 0.18), ('wales', 0.08), ('was', 0.08), ('wikipedia', 0.19), ('accuracy', 0.11), ('all', 0.11), ('allowing', 0.11), ('although', 0.11), ('anyone', 0.11), ('approached', 0.11), ('best', 0.11), ('biggest', 0.11), ('britannica', 0.22), ('comparing', 0.11), ('contentious', 0.11), ('critics', 0.11), ('door', 0.11), ('edit', 0.08), ('encyclop√¶dia', 0.11), ('fared', 0.11), ('focused', 0.11), ('following', 0.11), ('found', 0.11), ('from', 0.08), ('had', 0.11), ('hard', 0.11), ('have', 0.08), ('issues', 0.11), ('it', 0.08), ('level', 0.11), ('made', 0.11), ('might', 0.11), ('nature', 0.11), ('not', 0.11), ('one', 0.08), ('open', 0.11), ('or', 0.16), ('peer', 0.11), ('policy', 0.11), ('possibly', 0.11), ('published', 0.11), ('random', 0.11), ('review', 0.11), ('sampling', 0.11), ('science', 0.22), ('similar', 0.08), ('so', 0.08), ('social', 0.22), ('stated', 0.11), ('study', 0.11), ('suggested', 0.11), ('testament', 0.11), ('that', 0.21), ('time', 0.08), ('to', 0.07), ('vision', 0.11), ('well', 0.11), ('world', 0.11), ('year', 0.11)], [], [('and', 0.12), ('articles', 0.06), ('by', 0.14), ('english', 0.07), ('for', 0.22), ('in', 0.06), ('its', 0.06), ('language', 0.07), ('of', 0.11), ('on', 0.06), ('the', 0.05), ('wikipedia', 0.22), ('edit', 0.07), ('have', 0.07), ('it', 0.07), ('similar', 0.07), ('that', 0.04), ('to', 0.09), ('also', 0.09), ('announced', 0.19), ('are', 0.09), ('been', 0.28), ('being', 0.09), ('bias', 0.19), ('black', 0.09), ('controversial', 0.09), ('coverage', 0.09), ('criticized', 0.28), ('detect', 0.09), ('dominant', 0.09), ('editors', 0.19), ('edwin', 0.09), ('encourage', 0.09), ('exhibiting', 0.09), ('facebook', 0.09), ('fake', 0.09), ('falsehoods', 0.09), ('female', 0.09), ('gender', 0.09), ('half', 0.09), ('has', 0.28), ('held', 0.09), ('help', 0.09), ('however', 0.09), ('increase', 0.09), ('links', 0.09), ('majority', 0.09), ('male', 0.09), ('manipulation', 0.09), ('mixture', 0.09), ('news', 0.07), ('particularly', 0.09), ('plan', 0.09), ('presenting', 0.09), ('readers', 0.09), ('related', 0.09), ('some', 0.07), ('spin', 0.09), ('subject', 0.09), ('suggesting', 0.09), ('systemic', 0.09), ('thons', 0.09), ('topics', 0.19), ('truth', 0.19), ('version', 0.09), ('where', 0.09), ('women', 0.09), ('would', 0.09), ('youtube', 0.09)], [('is', 0.09), ('the', 0.13), ('to', 0.23), ('able', 0.24), ('be', 0.24), ('create', 0.18), ('dictionary', 0.14), ('entire', 0.18), ('file', 0.18), ('having', 0.24), ('input', 0.24), ('large', 0.24), ('load', 0.24), ('need', 0.24), ('now', 0.24), ('object', 0.24), ('text', 0.35), ('when', 0.24), ('without', 0.18), ('you', 0.18), ('your', 0.24)], [], [('and', 0.07), ('in', 0.07), ('is', 0.08), ('the', 0.27), ('one', 0.15), ('that', 0.1), ('time', 0.15), ('to', 0.07), ('news', 0.15), ('dictionary', 0.12), ('entire', 0.15), ('file', 0.15), ('text', 0.3), ('without', 0.15), ('you', 0.15), ('at', 0.2), ('do', 0.2), ('gensim', 0.2), ('good', 0.2), ('how', 0.15), ('into', 0.2), ('let', 0.12), ('lets', 0.2), ('line', 0.2), ('loading', 0.2), ('memory', 0.2), ('next', 0.2), ('read', 0.2), ('sections', 0.2), ('see', 0.15), ('system', 0.2), ('update', 0.2)], [], [('in', 0.12), ('some', 0.27), ('let', 0.21), ('before', 0.36), ('but', 0.36), ('get', 0.27), ('jargon', 0.36), ('nlp', 0.36), ('understand', 0.36), ('we', 0.36)], [], [('and', 0.08), ('as', 0.19), ('for', 0.08), ('in', 0.04), ('is', 0.15), ('its', 0.08), ('of', 0.2), ('the', 0.03), ('words', 0.19), ('or', 0.1), ('that', 0.12), ('to', 0.04), ('bag', 0.13), ('can', 0.13), ('collection', 0.13), ('contains', 0.13), ('corpus', 0.26), ('count', 0.13), ('document', 0.39), ('documents', 0.13), ('each', 0.26), ('frequency', 0.13), ('id', 0.13), ('information', 0.13), ('lost', 0.13), ('means', 0.13), ('order', 0.13), ('paragraph', 0.13), ('refer', 0.13), ('result', 0.13), ('sentence', 0.13), ('token', 0.13), ('typically', 0.39), ('word', 0.26)], [], [('and', 0.09), ('is', 0.11), ('of', 0.11), ('the', 0.07), ('from', 0.2), ('so', 0.2), ('to', 0.09), ('create', 0.2), ('dictionary', 0.16), ('how', 0.2), ('let', 0.16), ('see', 0.2), ('get', 0.2), ('clear', 0.27), ('everything', 0.27), ('far', 0.27), ('hands', 0.27), ('if', 0.27), ('list', 0.27), ('our', 0.27), ('sentences', 0.27), ('wet', 0.27)]]\n"
     ]
    }
   ],
   "source": [
    "TfidfModel_details = [[(master_dict[id], np.round(freq,decimals=2)) for id, freq in line] for line in tfidf[Corpus]]\n",
    "print(TfidfModel_details)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
